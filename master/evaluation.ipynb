{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the Dataset 1) codec 2) zxing 3) swarm 4) weaver 5) swt: codec\n",
      "Dataset(name='codec', root=PosixPath('/home/nimit/IITK/IR_CS657A/g18-project/master/../data/CODEC'), src=PosixPath('/home/nimit/IITK/IR_CS657A/g18-project/master/../data/CODEC/gitrepo'), bug_repo=PosixPath('/home/nimit/IITK/IR_CS657A/g18-project/master/../data/CODEC/bugrepo/repository.xml'))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import optimize\n",
    "from datasets import DATASET, RESULTS_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the rank score of different algorithms\n",
    "def combine_rank_scores(coeffs, *rank_scores):\n",
    "    \n",
    "    # print(type(rank_scores))\n",
    "    final_score = []\n",
    "    for scores in zip(*rank_scores):\n",
    "        combined_score = coeffs @ np.array(scores)\n",
    "        final_score.append(combined_score)\n",
    "\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cost(coeffs, src_files, bug_reports, *rank_scores):\n",
    "\n",
    "    final_scores = combine_rank_scores(coeffs, *rank_scores)\n",
    "\n",
    "    mrr = []\n",
    "    mean_avgp = []\n",
    "    ans = 0\n",
    "\n",
    "    for i, report in enumerate(bug_reports.items()):\n",
    "\n",
    "        # getting the source files from the simis indices\n",
    "        src_ranks, _ = zip(*sorted(zip(src_files.keys(), final_scores[i]), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "        # getting reported fixed files\n",
    "        fixed_files = report[1].fixed_files\n",
    "\n",
    "        # getting the ranks of reported fixed files\n",
    "\n",
    "        relevant_ranks = []\n",
    "        for fixed in fixed_files:\n",
    "            if fixed in src_ranks:\n",
    "                relevant_ranks.append(src_ranks.index(fixed)+1)\n",
    "\n",
    "        relevant_ranks = sorted(relevant_ranks)\n",
    "\n",
    "        # If required fixed files are not in the codebase anymore\n",
    "        if not relevant_ranks:\n",
    "            mrr.append(0)\n",
    "            mean_avgp.append(0)\n",
    "            continue\n",
    "\n",
    "        # MRR\n",
    "        min_rank = relevant_ranks[0]\n",
    "        mrr.append(1 / min_rank)\n",
    "\n",
    "        # MAP\n",
    "        for j, rank in enumerate(relevant_ranks):\n",
    "            t = len(relevant_ranks[:j + 1]) / rank\n",
    "            mean_avgp.append(np.mean(t))\n",
    "            \n",
    "    ans = -1 * (np.mean(mrr) + np.mean(mean_avgp))\n",
    "\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating linear combination parameters w1,w2,w3.....wn\n",
    "def estimate_params(src_files, bug_reports, *rank_scores):\n",
    "    \n",
    "\n",
    "    res = optimize.differential_evolution(\n",
    "        cost, bounds=[(0, 1)] * len(rank_scores),\n",
    "        args=(src_files, bug_reports, *rank_scores),\n",
    "        strategy='randtobest1exp', polish=True, seed=458711526\n",
    "    )\n",
    "\n",
    "    return res.x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for finding mrr and map\n",
    "def evaluate(src_files, bug_reports, coeffs, *rank_scores):\n",
    "\n",
    "    final_scores = combine_rank_scores(coeffs, *rank_scores)\n",
    "    # print(final_scores)\n",
    "\n",
    "    # Writer for the output file\n",
    "    result_file = open(RESULTS_ROOT / f'{DATASET.name}_output.jsonl', 'w')\n",
    "\n",
    "    top_n = (1, 3, 5, 10, 15)\n",
    "    top_n_rank = [0] * len(top_n)\n",
    "    mrr = []\n",
    "    mean_avgp = []\n",
    "\n",
    "    precision_at_n = [[] for _ in top_n]\n",
    "    recall_at_n = [[] for _ in top_n]\n",
    "    f_measure_at_n = [[] for _ in top_n]\n",
    "\n",
    "    for i, (bug_id, report) in enumerate(bug_reports.items()):\n",
    "        # print(i,(bug_id,report))\n",
    "\n",
    "        # Finding source codes from the simis indices\n",
    "        src_ranks, _ = zip(*sorted(zip(src_files.keys(), final_scores[i]),key=operator.itemgetter(1), reverse=True))\n",
    "        # print(src_ranks)\n",
    "\n",
    "        # Getting reported fixed files\n",
    "        fixed_files = report.fixed_files\n",
    "        # print(fixed_files)\n",
    "\n",
    "        # Iterating over top n\n",
    "        for k, rank in enumerate(top_n):\n",
    "            \n",
    "            hit = set(src_ranks[:rank]) & set(fixed_files)\n",
    "            # print(hit)\n",
    "\n",
    "            # Computing top n rank\n",
    "            if hit:\n",
    "                top_n_rank[k] += 1\n",
    "\n",
    "            # Computing precision and recall at n\n",
    "            if not hit:\n",
    "                precision_at_n[k].append(0)\n",
    "            else:\n",
    "                precision_at_n[k].append(len(hit) / len(src_ranks[:rank]))\n",
    "\n",
    "            recall_at_n[k].append(len(hit) / len(fixed_files))\n",
    "\n",
    "            if not (precision_at_n[k][i] + recall_at_n[k][i]):\n",
    "                f_measure_at_n[k].append(0)\n",
    "            else:\n",
    "                f_measure_at_n[k].append(2 * (precision_at_n[k][i] * recall_at_n[k][i]) / (precision_at_n[k][i] + recall_at_n[k][i]))\n",
    "\n",
    "        # getting the ranks of reported fixed files\n",
    "        relevant_ranks = sorted(src_ranks.index(fixed) + 1 for fixed in fixed_files if fixed in src_ranks)\n",
    "        # print(relevant_ranks)\n",
    "\n",
    "        # If required fixed files are not in the codebase anymore\n",
    "        if not relevant_ranks:\n",
    "            mrr.append(0)\n",
    "            mean_avgp.append(0)\n",
    "            continue\n",
    "\n",
    "        # MRR\n",
    "        min_rank = relevant_ranks[0]\n",
    "\n",
    "        # print(min_rank)\n",
    "        \n",
    "        mrr.append(1 / min_rank)\n",
    "\n",
    "        # MAP\n",
    "\n",
    "        mean_avgp = []\n",
    "        for j, rank in enumerate(relevant_ranks):\n",
    "            # print(j,rank)\n",
    "            t = len(relevant_ranks[:j + 1]) / rank\n",
    "            mean_avgp.append(np.mean(t))\n",
    "\n",
    "        result_file.write(json.dumps({'bug_id': bug_id, 'src_ranks': src_ranks}) + '\\n'\n",
    "        )\n",
    "\n",
    "    result_file.close()\n",
    "\n",
    "    first = top_n_rank\n",
    "    second = []\n",
    "    for x in top_n_rank:\n",
    "        second.append(x/len(bug_reports))\n",
    "\n",
    "    third = np.mean(mrr)\n",
    "    # print(DATASET.name,precision_at_n)\n",
    "    \n",
    "    fourth = np.mean(mean_avgp)\n",
    "    some = ['Overall_score',DATASET.name,np.mean(mrr),np.mean(mean_avgp)]\n",
    "    df.loc[len(df)] = some\n",
    "    fifth = np.mean(precision_at_n, axis=1).tolist()\n",
    "    sixth = np.mean(recall_at_n, axis=1).tolist()\n",
    "    seventh = np.mean(f_measure_at_n, axis=1).tolist()\n",
    "\n",
    "    return (first, second, third, fourth, fifth, sixth, seventh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nimit/IITK/IR_CS657A/g18-project/master/table.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path+=\"/table.csv\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    with open(DATASET.root / 'preprocessed_src.pickle', 'rb') as file:\n",
    "        src_files = pickle.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'preprocessed_reports.pickle', 'rb') as file:\n",
    "        bug_reports = pickle.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'token_matching.json', 'r') as file:\n",
    "        token_matching_score = json.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'vsm_similarity.json', 'r') as file:\n",
    "        vsm_similarity_score = json.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'stack_trace.json', 'r') as file:\n",
    "        stack_trace_score = json.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'semantic_similarity.json', 'r') as file:\n",
    "        semantic_similarity_score = json.load(file)\n",
    "\n",
    "    with open(DATASET.root / 'fixed_bug_reports.json', 'r') as file:\n",
    "        fixed_bug_reports_score = json.load(file)\n",
    "\n",
    "    params = estimate_params(\n",
    "        src_files,\n",
    "        bug_reports,\n",
    "        vsm_similarity_score,\n",
    "        token_matching_score,\n",
    "        fixed_bug_reports_score,\n",
    "        semantic_similarity_score,\n",
    "        stack_trace_score\n",
    "    )\n",
    "\n",
    "    results = evaluate(\n",
    "        src_files,\n",
    "        bug_reports,\n",
    "        params,\n",
    "        vsm_similarity_score,\n",
    "        token_matching_score,\n",
    "        fixed_bug_reports_score,\n",
    "        semantic_similarity_score,\n",
    "        stack_trace_score\n",
    "    )\n",
    "\n",
    "    # print(f'{params = }')\n",
    "    # print('Top N Rank:', results[0])\n",
    "    # print('Top N Rank %:', results[1])\n",
    "    print('MRR:', results[2])\n",
    "    print('MAP:', results[3])\n",
    "    #some = ['Overall_score',DATASET.name,np.mean(mrr),np.mean(mean_avgp)]\n",
    "    #df.loc[len(df)]=some\n",
    "\n",
    "# Uncomment these for precision, recall, and f-measure results\n",
    "    # print('Precision@N:', results[4])\n",
    "    # print('Recall@N:', results[5])\n",
    "    # print('F-measure@N:', results[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.5586769776908633\n",
      "MAP: 0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.193994</td>\n",
       "      <td>0.830590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>zxing</td>\n",
       "      <td>0.557674</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>swarm</td>\n",
       "      <td>0.471207</td>\n",
       "      <td>0.992221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>weaver</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.984663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>swt</td>\n",
       "      <td>0.217095</td>\n",
       "      <td>0.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>zxing</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.118728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>swarm</td>\n",
       "      <td>0.421716</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>weaver</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.255208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>swt</td>\n",
       "      <td>0.757346</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Semantic_similarity_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.981605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Semantic_similarity_score</td>\n",
       "      <td>zxing</td>\n",
       "      <td>0.134890</td>\n",
       "      <td>0.997659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Semantic_similarity_score</td>\n",
       "      <td>swarm</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.998046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Semantic_similarity_score</td>\n",
       "      <td>weaver</td>\n",
       "      <td>0.079004</td>\n",
       "      <td>0.982911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Semantic_similarity_score</td>\n",
       "      <td>swt</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.997406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VSM_similarity_score</td>\n",
       "      <td>weaver</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.639413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VSM_similarity_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.657788</td>\n",
       "      <td>0.808599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VSM_similarity_score</td>\n",
       "      <td>swarm</td>\n",
       "      <td>0.605295</td>\n",
       "      <td>0.654434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VSM_similarity_score</td>\n",
       "      <td>zxing</td>\n",
       "      <td>0.073199</td>\n",
       "      <td>0.896099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>VSM_similarity_score</td>\n",
       "      <td>swt</td>\n",
       "      <td>0.040685</td>\n",
       "      <td>0.786150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Token_matching_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.101858</td>\n",
       "      <td>0.830590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.558677</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Overall_score</td>\n",
       "      <td>codec</td>\n",
       "      <td>0.558677</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model Dataset       MRR       MAP\n",
       "0        Token_matching_score   codec  0.193994  0.830590\n",
       "1        Token_matching_score   zxing  0.557674  0.994220\n",
       "2        Token_matching_score   swarm  0.471207  0.992221\n",
       "3        Token_matching_score  weaver  0.081633  0.984663\n",
       "4        Token_matching_score     swt  0.217095  0.970400\n",
       "5               Overall_score   codec  0.845085  0.166667\n",
       "6               Overall_score   zxing  0.676000  0.118728\n",
       "7               Overall_score   swarm  0.421716  0.029412\n",
       "8               Overall_score  weaver  0.416667  0.255208\n",
       "9               Overall_score     swt  0.757346  1.000000\n",
       "10  Semantic_similarity_score   codec  0.028432  0.981605\n",
       "11  Semantic_similarity_score   zxing  0.134890  0.997659\n",
       "12  Semantic_similarity_score   swarm  0.019390  0.998046\n",
       "13  Semantic_similarity_score  weaver  0.079004  0.982911\n",
       "14  Semantic_similarity_score     swt  0.022470  0.997406\n",
       "15       VSM_similarity_score  weaver  0.750000  0.639413\n",
       "16       VSM_similarity_score   codec  0.657788  0.808599\n",
       "17       VSM_similarity_score   swarm  0.605295  0.654434\n",
       "18       VSM_similarity_score   zxing  0.073199  0.896099\n",
       "19       VSM_similarity_score     swt  0.040685  0.786150\n",
       "20       Token_matching_score   codec  0.101858  0.830590\n",
       "21              Overall_score   codec  0.558677  0.043478\n",
       "22              Overall_score   codec  0.558677  0.043478"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # df.drop_duplicates(subset = \"Dataset\" ,keep = \"first\", inplace = True)\n",
    "# df.to_csv(path,index=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f3b078249380ea762697f4f8f6aea77b3d6e43cbb1b18cbb73d8cde5aa597e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
