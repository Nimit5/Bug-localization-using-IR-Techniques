{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name='codec', root=WindowsPath('d:/IRProj/bug-localization-master/buglocalizer/../data/CODEC'), src=WindowsPath('d:/IRProj/bug-localization-master/buglocalizer/../data/CODEC/gitrepo'), bug_repo=WindowsPath('d:/IRProj/bug-localization-master/buglocalizer/../data/CODEC/bugrepo/repository.xml'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import inflection\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from datasets import DATASET\n",
    "from parsers import Parser\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#keywords in java\n",
    "\n",
    "java_keywords = set(['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n",
    "     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n",
    "     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n",
    "     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n",
    "     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n",
    "     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n",
    "     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for Preprocessing bug reports\n",
    "\n",
    "class ReportPreprocessing:\n",
    "    \n",
    "    __slots__ = ['bug_reports']\n",
    "\n",
    "    def __init__(self, bug_reports):\n",
    "        \n",
    "        self.bug_reports = bug_reports\n",
    "    \n",
    "    # Function to extract stack traces from bug reports\n",
    "    \n",
    "    def stack_traces_extract(self):\n",
    "\n",
    "        #Pattern for retrieving stack traces\n",
    "        \n",
    "        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n",
    "\n",
    "        # Signs of a true stack trace for checking the fetched pattern\n",
    "        \n",
    "        signs = [ 'Unknown Source','.java', 'Native Method']\n",
    "        \n",
    "        for report in self.bug_reports.values():\n",
    "            \n",
    "            candidate_for_st = re.findall(pattern, report.description)\n",
    "\n",
    "            #finding the actual stack trace out of all the possible candidates\n",
    "            st = [x for x in candidate_for_st if any(s in x[1] for s in signs)]\n",
    "            \n",
    "            report.stack_traces = st\n",
    "            \n",
    "    #Function for tokenizing bug reports\n",
    "    \n",
    "    def tokenize(self):\n",
    "        \n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = nltk.wordpunct_tokenize(report.summary)\n",
    "            report.description = nltk.wordpunct_tokenize(report.description)\n",
    "            \n",
    "     \n",
    "    #Function extracting specific pos tags from bug report's summary and description\n",
    "    \n",
    "    def pos_tagging(self):\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            \n",
    "            ps=['NN','VB']\n",
    "\n",
    "            # Tokenization and pos-tagging in report summary\n",
    "            \n",
    "            summ_token = nltk.word_tokenize(report.summary)\n",
    "            summ_pos = nltk.pos_tag(summ_token)\n",
    "            report.pos_tagged_summary = [token for token, pos in summ_pos if pos in ps]\n",
    "            \n",
    "            # Tokenization and pos-tagging in report description\n",
    "            \n",
    "            desc_token = nltk.word_tokenize(report.description)\n",
    "            desc_pos = nltk.pos_tag(desc_token)\n",
    "            report.pos_tagged_description = [token for token, pos in desc_pos if pos in ps]\n",
    "                \n",
    "     \n",
    "    #Function for splitting camelcase\n",
    "    def _split_camelcase(self, tokens):\n",
    "\n",
    "        returning_tokens = tokens\n",
    "\n",
    "        for token in tokens:\n",
    "            \n",
    "            split_tokens = re.split(r'[{string.punctuation}]+', token)\n",
    "            \n",
    "            if len(split_tokens) > 1:\n",
    "                returning_tokens.remove(token)\n",
    "                \n",
    "                # Camel case detection for new tokens\n",
    "                for st in split_tokens:\n",
    "                    camel_split = inflection.underscore(st).split('_')\n",
    "                    if len(camel_split) > 1:\n",
    "                        returning_tokens.append(st)\n",
    "                        returning_tokens += camel_split\n",
    "                    else:\n",
    "                        returning_tokens.append(st)\n",
    "            else:\n",
    "                camel_split = inflection.underscore(token).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens += camel_split\n",
    "\n",
    "        return returning_tokens\n",
    "     \n",
    "    #Function for applying __split__camelcase\n",
    "    \n",
    "    def split_camelcase_apply(self):\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            \n",
    "            report.summary = self._split_camelcase(report.summary)\n",
    "            report.description = self._split_camelcase(report.description)\n",
    "            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n",
    "            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n",
    "    \n",
    "    #Function for removing punctuation, numbers and converting text into lowercase\n",
    "    \n",
    "    def clean(self):\n",
    "       \n",
    "\n",
    "        # Building a translate table for punctuation and number removal\n",
    "        \n",
    "        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n",
    "            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n",
    "            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n",
    "            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n",
    "            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n",
    "            report.description = [token.lower() for token in desc_punctnum_rem if token]\n",
    "            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n",
    "            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n",
    "\n",
    "    #Function for removing stopwords and javakeywords\n",
    "    \n",
    "    def remove_stopwords_keywords(self):\n",
    "        \n",
    "                    \n",
    "        for report in self.bug_reports.values():\n",
    "            \n",
    "            #Removal in Report Summary\n",
    "            report.summary = [token for token in report.summary if token not in stop_words]\n",
    "            report.summary = [token for token in report.summary if token not in java_keywords]\n",
    "            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n",
    "            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n",
    "            \n",
    "            #Removal in Report Description \n",
    "            report.description = [token for token in report.description if token not in stop_words]\n",
    "            report.description = [token for token in report.description if token not in java_keywords]\n",
    "            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n",
    "            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n",
    "   \n",
    "    #Function for performing the stemming in tokens using porter stemmer\n",
    "    \n",
    "    def stem(self):\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in report.summary],report.summary]))\n",
    "\n",
    "            report.description = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in report.description],report.description]))\n",
    "\n",
    "            report.pos_tagged_summary = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in report.pos_tagged_summary],report.pos_tagged_summary]))\n",
    "\n",
    "            report.pos_tagged_description = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in report.pos_tagged_description],report.pos_tagged_description]))\n",
    "\n",
    "    #Function that calls all the function above for doing complete preprocessing\n",
    "    \n",
    "    def preprocess(self):\n",
    "\n",
    "        self.stack_traces_extract()\n",
    "        self.pos_tagging()\n",
    "        self.tokenize()\n",
    "        self.split_camelcase_apply()\n",
    "        self.clean()\n",
    "        self.remove_stopwords_keywords()\n",
    "        self.stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=['NN','VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for preprocessing source code\n",
    "\n",
    "class SrcPreprocessing:\n",
    "    \n",
    "    \n",
    "    \n",
    "    __slots__ = ['src_files']\n",
    "\n",
    "    def __init__(self, src_files):\n",
    "        self.src_files = src_files\n",
    "    \n",
    "    #Function for extracting specific pos tags\n",
    "    \n",
    "    def pos_tagging(self):\n",
    "        \n",
    "        for src in self.src_files.values():\n",
    "\n",
    "            # Tokenizing word and doing pos tagging\n",
    "            \n",
    "            comments_tok = nltk.word_tokenize(src.comments)\n",
    "            comments_pos = nltk.pos_tag(comments_tok)\n",
    "\n",
    "            src.pos_tagged_comments = [token for token, pos in comments_pos if pos in ps]\n",
    "            \n",
    "    #Function for tokenizing source code\n",
    "    \n",
    "    def tokenize(self):\n",
    "\n",
    "        for src in self.src_files.values():\n",
    "            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n",
    "            src.comments = nltk.wordpunct_tokenize(src.comments)\n",
    "            # print(src.all_content,src.comments)\n",
    "     \n",
    "    #Function for splitting camelcase\n",
    "    \n",
    "    def _split_camelcase(self, tokens):\n",
    "\n",
    "        # Copy tokens\n",
    "        returning_tokens = tokens[:]\n",
    "\n",
    "        for token in tokens:\n",
    "            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "\n",
    "            # If token is split into some other tokens\n",
    "            if len(split_tokens) > 1:\n",
    "                returning_tokens.remove(token)\n",
    "                # Camel case detection for new tokens\n",
    "                for st in split_tokens:\n",
    "                    camel_split = inflection.underscore(st).split('_')\n",
    "                    if len(camel_split) > 1:\n",
    "                        returning_tokens.append(st)\n",
    "                        returning_tokens += camel_split\n",
    "                    else:\n",
    "                        returning_tokens.append(st)\n",
    "            else:\n",
    "                camel_split = inflection.underscore(token).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens += camel_split\n",
    "        # print(returning_tokens)\n",
    "\n",
    "        return returning_tokens\n",
    "    \n",
    "    #Function for applying __split__camelcase\n",
    "    \n",
    "    def split_camelcase_apply(self):\n",
    "\n",
    "        for src in self.src_files.values():\n",
    "            src.all_content = self._split_camelcase(src.all_content)\n",
    "            src.comments = self._split_camelcase(src.comments)\n",
    "            src.class_names = self._split_camelcase(src.class_names)\n",
    "            src.attributes = self._split_camelcase(src.attributes)\n",
    "            src.method_names = self._split_camelcase(src.method_names)\n",
    "            src.variables = self._split_camelcase(src.variables)\n",
    "            src.file_name = self._split_camelcase(src.file_name)\n",
    "            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n",
    "\n",
    "\n",
    "     #Function for removing punctuation, numbers and converting text into lowercase\n",
    "    \n",
    "    def clean(self):\n",
    "        \n",
    "        # Building a translate table for punctuation and number removal\n",
    "        \n",
    "        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n",
    "\n",
    "        for src in self.src_files.values():\n",
    "            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n",
    "            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n",
    "            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n",
    "            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n",
    "            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n",
    "            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n",
    "            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n",
    "            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n",
    "            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n",
    "            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n",
    "            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n",
    "            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n",
    "            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n",
    "            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n",
    "            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n",
    "            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n",
    "     \n",
    "    #Function for removing stopwords and javakeywords\n",
    "    \n",
    "    def remove_stopwords_keywords(self):\n",
    "        \n",
    "        for src in self.src_files.values():\n",
    "            src.all_content = [token for token in src.all_content if token not in stop_words]\n",
    "            src.all_content = [token for token in src.all_content if token not in java_keywords]\n",
    "            src.comments = [token for token in src.comments if token not in stop_words]\n",
    "            src.comments = [token for token in src.comments if token not in java_keywords]\n",
    "            src.class_names = [token for token in src.class_names if token not in stop_words]\n",
    "            src.class_names = [token for token in src.class_names if token not in java_keywords]\n",
    "            src.attributes = [token for token in src.attributes if token not in stop_words]\n",
    "            src.attributes = [token for token in src.attributes if token not in java_keywords]\n",
    "            src.method_names = [token for token in src.method_names if token not in stop_words]\n",
    "            src.method_names = [token for token in src.method_names if token not in java_keywords]\n",
    "            src.variables = [token for token in src.variables if token not in stop_words]\n",
    "            src.variables = [token for token in src.variables if token not in java_keywords]\n",
    "            src.file_name = [token for token in src.file_name if token not in stop_words]\n",
    "            src.file_name = [token for token in src.file_name if token not in java_keywords]\n",
    "            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n",
    "            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n",
    "            \n",
    "    #Function for performing the stemming in tokens using porter stemmer\n",
    "    def stem(self):\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        for src in self.src_files.values():\n",
    "            src.all_content = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.all_content],src.all_content]))\n",
    "\n",
    "            src.comments = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.comments],src.comments]))\n",
    "\n",
    "            src.class_names = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.class_names], src.class_names]))\n",
    "\n",
    "            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes],src.attributes]))\n",
    "\n",
    "            src.method_names = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.method_names],src.method_names]))\n",
    "\n",
    "            src.variables = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.variables],src.variables]))\n",
    "\n",
    "            src.file_name = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.file_name],src.file_name]))\n",
    "\n",
    "            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in src.pos_tagged_comments],src.pos_tagged_comments]))\n",
    "\n",
    "     #Function that calls all the function above for doing complete preprocessing\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \n",
    "        self.pos_tagging()\n",
    "        self.tokenize()\n",
    "        self.split_camelcase_apply()\n",
    "        self.clean()\n",
    "        self.remove_stopwords_keywords()\n",
    "        self.stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    parser = Parser(DATASET)\n",
    "    src_prep = SrcPreprocessing(parser.src_parser())\n",
    "    src_prep.preprocess()\n",
    "    with open(DATASET.root / 'preprocessed_src.pickle', 'wb') as file:\n",
    "        pickle.dump(src_prep.src_files, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(src_prep.src_files)\n",
    "    report_prep = ReportPreprocessing(parser.report_parser())\n",
    "    report_prep.preprocess()\n",
    "    with open(DATASET.root / 'preprocessed_reports.pickle', 'wb') as file:\n",
    "        pickle.dump(report_prep.bug_reports, file,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('org.apache.commons.codec.BinaryDecoder.java', <parsers.SourceFile object at 0x000002D46A6A46D0>), ('org.apache.commons.codec.BinaryEncoder.java', <parsers.SourceFile object at 0x000002D46A6A4660>), ('org.apache.commons.codec.CharEncoding.java', <parsers.SourceFile object at 0x000002D46A6A4740>), ('org.apache.commons.codec.Charsets.java', <parsers.SourceFile object at 0x000002D46A6A47B0>), ('org.apache.commons.codec.Decoder.java', <parsers.SourceFile object at 0x000002D46A6A4820>), ('org.apache.commons.codec.DecoderException.java', <parsers.SourceFile object at 0x000002D46A6A4BA0>), ('org.apache.commons.codec.Encoder.java', <parsers.SourceFile object at 0x000002D46A6A4C80>), ('org.apache.commons.codec.EncoderException.java', <parsers.SourceFile object at 0x000002D46A6A4890>), ('org.apache.commons.codec.StringDecoder.java', <parsers.SourceFile object at 0x000002D46A6A4900>), ('org.apache.commons.codec.StringEncoder.java', <parsers.SourceFile object at 0x000002D46A6A4970>), ('org.apache.commons.codec.StringEncoderComparator.java', <parsers.SourceFile object at 0x000002D46A6A4A50>), ('org.apache.commons.codec.binary.Base32.java', <parsers.SourceFile object at 0x000002D46A6A4AC0>), ('org.apache.commons.codec.binary.Base32InputStream.java', <parsers.SourceFile object at 0x000002D46A6A4DD0>), ('org.apache.commons.codec.binary.Base32OutputStream.java', <parsers.SourceFile object at 0x000002D46A6A4EB0>), ('org.apache.commons.codec.binary.Base64.java', <parsers.SourceFile object at 0x000002D46A5670D0>), ('org.apache.commons.codec.binary.Base64InputStream.java', <parsers.SourceFile object at 0x000002D46A5678B0>), ('org.apache.commons.codec.binary.Base64OutputStream.java', <parsers.SourceFile object at 0x000002D46A6A4E40>), ('org.apache.commons.codec.binary.BaseNCodec.java', <parsers.SourceFile object at 0x000002D46A567140>), ('org.apache.commons.codec.binary.BaseNCodecInputStream.java', <parsers.SourceFile object at 0x000002D46A6A5460>), ('org.apache.commons.codec.binary.BaseNCodecOutputStream.java', <parsers.SourceFile object at 0x000002D46A6A5620>), ('org.apache.commons.codec.binary.BinaryCodec.java', <parsers.SourceFile object at 0x000002D46A6A5700>), ('org.apache.commons.codec.binary.Hex.java', <parsers.SourceFile object at 0x000002D46A6A5690>), ('org.apache.commons.codec.binary.StringUtils.java', <parsers.SourceFile object at 0x000002D46A6A5150>), ('org.apache.commons.codec.digest.B64.java', <parsers.SourceFile object at 0x000002D46A6A50E0>), ('org.apache.commons.codec.digest.Crypt.java', <parsers.SourceFile object at 0x000002D46A6A4B30>), ('org.apache.commons.codec.digest.DigestUtils.java', <parsers.SourceFile object at 0x000002D46A6A55B0>), ('org.apache.commons.codec.digest.Md5Crypt.java', <parsers.SourceFile object at 0x000002D46A6A4F90>), ('org.apache.commons.codec.digest.MessageDigestAlgorithms.java', <parsers.SourceFile object at 0x000002D46A6A5000>), ('org.apache.commons.codec.digest.Sha2Crypt.java', <parsers.SourceFile object at 0x000002D46A6A57E0>), ('org.apache.commons.codec.digest.UnixCrypt.java', <parsers.SourceFile object at 0x000002D46A6A5850>), ('org.apache.commons.codec.language.AbstractCaverphone.java', <parsers.SourceFile object at 0x000002D46A6A5AF0>), ('org.apache.commons.codec.language.Caverphone.java', <parsers.SourceFile object at 0x000002D46A6A5BD0>), ('org.apache.commons.codec.language.Caverphone1.java', <parsers.SourceFile object at 0x000002D46A6A5CB0>), ('org.apache.commons.codec.language.Caverphone2.java', <parsers.SourceFile object at 0x000002D46A6A5930>), ('org.apache.commons.codec.language.ColognePhonetic.java', <parsers.SourceFile object at 0x000002D46A6A5D90>), ('org.apache.commons.codec.language.DoubleMetaphone.java', <parsers.SourceFile object at 0x000002D46A6A53F0>), ('org.apache.commons.codec.language.MatchRatingApproachEncoder.java', <parsers.SourceFile object at 0x000002D46A6A5E00>), ('org.apache.commons.codec.language.Metaphone.java', <parsers.SourceFile object at 0x000002D46A6A5E70>), ('org.apache.commons.codec.language.Nysiis.java', <parsers.SourceFile object at 0x000002D46A6A5380>), ('org.apache.commons.codec.language.RefinedSoundex.java', <parsers.SourceFile object at 0x000002D46A6A5F50>), ('org.apache.commons.codec.language.Soundex.java', <parsers.SourceFile object at 0x000002D46A6A5FC0>), ('org.apache.commons.codec.language.SoundexUtils.java', <parsers.SourceFile object at 0x000002D46A6A6570>), ('org.apache.commons.codec.language.bm.BeiderMorseEncoder.java', <parsers.SourceFile object at 0x000002D46A6A6110>), ('org.apache.commons.codec.language.bm.Lang.java', <parsers.SourceFile object at 0x000002D46A6A54D0>), ('org.apache.commons.codec.language.bm.Languages.java', <parsers.SourceFile object at 0x000002D46A6A66C0>), ('org.apache.commons.codec.language.bm.NameType.java', <parsers.SourceFile object at 0x000002D46A6A67A0>), ('org.apache.commons.codec.language.bm.PhoneticEngine.java', <parsers.SourceFile object at 0x000002D46A6A68F0>), ('org.apache.commons.codec.language.bm.ResourceConstants.java', <parsers.SourceFile object at 0x000002D46A6A6A40>), ('org.apache.commons.codec.language.bm.Rule.java', <parsers.SourceFile object at 0x000002D46A6A69D0>), ('org.apache.commons.codec.language.bm.RuleType.java', <parsers.SourceFile object at 0x000002D46A6A6880>), ('org.apache.commons.codec.net.BCodec.java', <parsers.SourceFile object at 0x000002D46A6A65E0>), ('org.apache.commons.codec.net.QCodec.java', <parsers.SourceFile object at 0x000002D46A6A6B90>), ('org.apache.commons.codec.net.QuotedPrintableCodec.java', <parsers.SourceFile object at 0x000002D46A6A6CE0>), ('org.apache.commons.codec.net.RFC1522Codec.java', <parsers.SourceFile object at 0x000002D46A6A6C70>), ('org.apache.commons.codec.net.URLCodec.java', <parsers.SourceFile object at 0x000002D46A6A6D50>), ('org.apache.commons.codec.net.Utils.java', <parsers.SourceFile object at 0x000002D46A6A6DC0>), ('org.apache.commons.codec.BinaryEncoderAbstractTest.java', <parsers.SourceFile object at 0x000002D46A6A6F10>), ('org.apache.commons.codec.CharEncodingTest.java', <parsers.SourceFile object at 0x000002D46A6A6B20>), ('org.apache.commons.codec.CharsetsTest.java', <parsers.SourceFile object at 0x000002D46A6A6F80>), ('org.apache.commons.codec.DecoderExceptionTest.java', <parsers.SourceFile object at 0x000002D46A6A6340>), ('org.apache.commons.codec.EncoderExceptionTest.java', <parsers.SourceFile object at 0x000002D46A6A7140>), ('org.apache.commons.codec.StringEncoderAbstractTest.java', <parsers.SourceFile object at 0x000002D46A6A7300>), ('org.apache.commons.codec.StringEncoderComparatorTest.java', <parsers.SourceFile object at 0x000002D46A6A7060>), ('org.apache.commons.codec.binary.Base32InputStreamTest.java', <parsers.SourceFile object at 0x000002D46A6A7610>), ('org.apache.commons.codec.binary.Base32OutputStreamTest.java', <parsers.SourceFile object at 0x000002D46A6A76F0>), ('org.apache.commons.codec.binary.Base32Test.java', <parsers.SourceFile object at 0x000002D46A6A7680>), ('org.apache.commons.codec.binary.Base32TestData.java', <parsers.SourceFile object at 0x000002D46A6A7840>), ('org.apache.commons.codec.binary.Base64Codec13Test.java', <parsers.SourceFile object at 0x000002D46A6A6650>), ('org.apache.commons.codec.binary.Base64InputStreamTest.java', <parsers.SourceFile object at 0x000002D46A6A7A00>), ('org.apache.commons.codec.binary.Base64OutputStreamTest.java', <parsers.SourceFile object at 0x000002D46A6A7990>), ('org.apache.commons.codec.binary.Base64Test.java', <parsers.SourceFile object at 0x000002D46A6A7BC0>), ('org.apache.commons.codec.binary.Base64TestData.java', <parsers.SourceFile object at 0x000002D46A6A7A70>), ('org.apache.commons.codec.binary.BaseNCodecTest.java', <parsers.SourceFile object at 0x000002D46A6A7920>), ('org.apache.commons.codec.binary.BinaryCodecTest.java', <parsers.SourceFile object at 0x000002D46A6A7D80>), ('org.apache.commons.codec.binary.Codec105ErrorInputStream.java', <parsers.SourceFile object at 0x000002D46A6A74C0>), ('org.apache.commons.codec.binary.HexTest.java', <parsers.SourceFile object at 0x000002D46A6A7450>), ('org.apache.commons.codec.binary.StringUtilsTest.java', <parsers.SourceFile object at 0x000002D46A6A7DF0>), ('org.apache.commons.codec.digest.Apr1CryptTest.java', <parsers.SourceFile object at 0x000002D46A6A7D10>), ('org.apache.commons.codec.digest.B64Test.java', <parsers.SourceFile object at 0x000002D46A6A7B50>), ('org.apache.commons.codec.digest.CryptTest.java', <parsers.SourceFile object at 0x000002D46A6A60A0>), ('org.apache.commons.codec.digest.DigestUtilsTest.java', <parsers.SourceFile object at 0x000002D46A7E85F0>), ('org.apache.commons.codec.digest.Md5CryptTest.java', <parsers.SourceFile object at 0x000002D46A7E86D0>), ('org.apache.commons.codec.digest.Sha256CryptTest.java', <parsers.SourceFile object at 0x000002D46A7E8660>), ('org.apache.commons.codec.digest.Sha2CryptTest.java', <parsers.SourceFile object at 0x000002D46A7E8B30>), ('org.apache.commons.codec.digest.Sha512CryptTest.java', <parsers.SourceFile object at 0x000002D46A7E8C10>), ('org.apache.commons.codec.digest.UnixCryptTest.java', <parsers.SourceFile object at 0x000002D46A7E8CF0>), ('org.apache.commons.codec.language.Caverphone1Test.java', <parsers.SourceFile object at 0x000002D46A7E8E40>), ('org.apache.commons.codec.language.Caverphone2Test.java', <parsers.SourceFile object at 0x000002D46A7E87B0>), ('org.apache.commons.codec.language.ColognePhoneticTest.java', <parsers.SourceFile object at 0x000002D46A7E8900>), ('org.apache.commons.codec.language.DoubleMetaphone2Test.java', <parsers.SourceFile object at 0x000002D46A7E8AC0>), ('org.apache.commons.codec.language.DoubleMetaphoneTest.java', <parsers.SourceFile object at 0x000002D46A7E83C0>), ('org.apache.commons.codec.language.MatchRatingApproachEncoderTest.java', <parsers.SourceFile object at 0x000002D46A7E8A50>), ('org.apache.commons.codec.language.MetaphoneTest.java', <parsers.SourceFile object at 0x000002D46A7E8040>), ('org.apache.commons.codec.language.NysiisTest.java', <parsers.SourceFile object at 0x000002D46A7E8350>), ('org.apache.commons.codec.language.RefinedSoundexTest.java', <parsers.SourceFile object at 0x000002D46A7E9150>), ('org.apache.commons.codec.language.SoundexTest.java', <parsers.SourceFile object at 0x000002D46A7E8430>), ('org.apache.commons.codec.language.bm.BeiderMorseEncoderTest.java', <parsers.SourceFile object at 0x000002D46A7E91C0>), ('org.apache.commons.codec.language.bm.CacheSubSequencePerformanceTest.java', <parsers.SourceFile object at 0x000002D46A7E9230>), ('org.apache.commons.codec.language.bm.LanguageGuessingTest.java', <parsers.SourceFile object at 0x000002D46A7E93F0>), ('org.apache.commons.codec.language.bm.PhoneticEnginePerformanceTest.java', <parsers.SourceFile object at 0x000002D46A7E8270>), ('org.apache.commons.codec.language.bm.PhoneticEngineRegressionTest.java', <parsers.SourceFile object at 0x000002D46A7E82E0>), ('org.apache.commons.codec.language.bm.PhoneticEngineTest.java', <parsers.SourceFile object at 0x000002D46A7E9850>), ('org.apache.commons.codec.language.bm.RuleTest.java', <parsers.SourceFile object at 0x000002D46A7E9930>), ('org.apache.commons.codec.net.BCodecTest.java', <parsers.SourceFile object at 0x000002D46A7E98C0>), ('org.apache.commons.codec.net.QCodecTest.java', <parsers.SourceFile object at 0x000002D46A7E99A0>), ('org.apache.commons.codec.net.QuotedPrintableCodecTest.java', <parsers.SourceFile object at 0x000002D46A7E9460>), ('org.apache.commons.codec.net.RFC1522CodecTest.java', <parsers.SourceFile object at 0x000002D46A7E92A0>), ('org.apache.commons.codec.net.URLCodecTest.java', <parsers.SourceFile object at 0x000002D46A7E9540>), ('org.apache.commons.codec.net.UtilsTest.java', <parsers.SourceFile object at 0x000002D46A7E9310>)])\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f3b078249380ea762697f4f8f6aea77b3d6e43cbb1b18cbb73d8cde5aa597e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
